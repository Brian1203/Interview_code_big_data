
结巴分词的过程:
1. 加载字典, 生成trie树
2. 给定待分词的句子, 使用正则获取连续的 中文字符和英文字符, 切分成 短语列表, 对每个短语使用DAG(查字典)和动态规划, 得到最大概率路径,
对DAG中那些没有在字典中查到的字, 组合成一个新的片段短语, 使用HMM模型进行分词, 也就是作者说的识别新词, 即识别字典外的新词.
3. 使用python的yield 语法生成一个词语生成器, 逐词语返回


1.dict.txt：语料库的有3列，第一列是词，第二列是词频，第三列是词性，2w多个词
2.trie 树其实就是一个嵌套的 dict
3.每一个sentence都会生成一个DAG，DAG的数据结构是dict + list。举一个例子
如 sentence 是 "国庆节我在研究结巴分词"，对应生成的DAG是这样的：
{0: [0, 1, 2], 1: [1], 2: [2], 3: [3], 4: [4], 5: [5, 6], 6: [6], 7: [7, 8], 8: [8], 9: [9, 10], 10: [10]}
4.所有的路径中找出一条路径使频度得分的总和最大，这是动态规划的一个典型应用.
5.对于未登录词的分词方法，使用HMM模型(二元模型)和用来解码HMM的维特比算法.
